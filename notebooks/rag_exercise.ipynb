{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0a3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paulamoure/project/rag-exercise-cohere/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d922bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere version: 5.15.0\n",
      "Pandas version: 2.2.3\n",
      "Numpy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import cohere  # SDK oficial de Cohere para usar sus modelos (embeddings, chat, etc.)\n",
    "import pandas as pd  # Manipulaci√≥n de datos tabulares (DataFrames) - ideal para CSVs\n",
    "import numpy as np  # Operaciones matem√°ticas y arrays - necesario para embeddings\n",
    "import os  # Interactuar con el sistema operativo (variables de entorno, archivos)\n",
    "from typing import List, Dict  # Type hints para mejor c√≥digo (List[str], Dict[str, Any])\n",
    "import json  # Leer/escribir archivos JSON - √∫til para guardar embeddings\n",
    "\n",
    "# Verificar versiones\n",
    "print(f\"Cohere version: {cohere.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente Cohere configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "# Configurar API Key de forma segura\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Cargar API key desde variable de entorno\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"COHERE_API_KEY no encontrada en .env\")\n",
    "\n",
    "co = cohere.Client(api_key)\n",
    "print(\"Cliente Cohere configurado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32de27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n exitosa. Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Probar conexi√≥n con Cohere\n",
    "try:\n",
    "    # Test simple con un embedding peque√±o\n",
    "    response = co.embed(texts=[\"Hello world\"], model=\"embed-english-v3.0\", input_type=\"search_document\")\n",
    "    print(f\"‚úÖ Conexi√≥n exitosa. Embedding dimension: {len(response.embeddings[0])}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a975572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creado con 8 documentos\n",
      "0: Cohere provides large language models and NLP tools for businesses.\n",
      "1: RAG combines retrieval of relevant documents with text generation.\n",
      "2: Vector databases store embeddings for fast similarity search.\n",
      "3: Semantic search finds documents based on meaning, not just keywords.\n",
      "4: Embeddings are numerical representations of text in high-dimensional space.\n",
      "5: Machine learning models can understand context and generate human-like text.\n",
      "6: Natural language processing helps computers understand human language.\n",
      "7: AI assistants use retrieval-augmented generation for accurate responses.\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset de documentos de ejemplo\n",
    "documents = [\n",
    "    \"Cohere provides large language models and NLP tools for businesses.\",\n",
    "    \"RAG combines retrieval of relevant documents with text generation.\",\n",
    "    \"Vector databases store embeddings for fast similarity search.\",\n",
    "    \"Semantic search finds documents based on meaning, not just keywords.\",\n",
    "    \"Embeddings are numerical representations of text in high-dimensional space.\",\n",
    "    \"Machine learning models can understand context and generate human-like text.\",\n",
    "    \"Natural language processing helps computers understand human language.\",\n",
    "    \"AI assistants use retrieval-augmented generation for accurate responses.\"\n",
    "]\n",
    "\n",
    "print(f\"Dataset creado con {len(documents)} documentos\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"{i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89fd73e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando embeddings para los documentos...\n",
      "Embeddings generados:\n",
      "- N√∫mero de documentos: 8\n",
      "- Dimensi√≥n de embeddings: 1024\n",
      "- Shape total: (8, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Generar embeddings para los documentos\n",
    "print(\"Generando embeddings para los documentos...\")\n",
    "\n",
    "# Crear embeddings para todos los documentos\n",
    "doc_embeddings = co.embed(\n",
    "    texts=documents,\n",
    "    model=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\"\n",
    ")\n",
    "\n",
    "# Convertir a numpy array para facilitar c√°lculos\n",
    "embeddings_array = np.array(doc_embeddings.embeddings)\n",
    "\n",
    "print(f\"Embeddings generados:\")\n",
    "print(f\"- N√∫mero de documentos: {len(documents)}\")\n",
    "print(f\"- Dimensi√≥n de embeddings: {embeddings_array.shape[1]}\")\n",
    "print(f\"- Shape total: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5736541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n de b√∫squeda sem√°ntica creada\n"
     ]
    }
   ],
   "source": [
    "# Implementar b√∫squeda sem√°ntica\n",
    "def semantic_search(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Busca los documentos m√°s relevantes para una query usando similitud coseno\n",
    "    \"\"\"\n",
    "    # Generar embedding para la query\n",
    "    query_embedding = co.embed(\n",
    "        texts=[query],\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_query\"\n",
    "    )\n",
    "\n",
    "    query_vector = np.array(query_embedding.embeddings[0])\n",
    "\n",
    "    # Calcular similitud coseno con todos los documentos\n",
    "    similarities = np.dot(embeddings_array, query_vector) / (\n",
    "        np.linalg.norm(embeddings_array, axis=1) * np.linalg.norm(query_vector)\n",
    "    )\n",
    "\n",
    "    # Obtener los top_k m√°s similares\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"document\": documents[idx],\n",
    "            \"similarity\": similarities[idx],\n",
    "            \"index\": idx\n",
    "        })\n",
    "    return results\n",
    "print(\"Funci√≥n de b√∫squeda sem√°ntica creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c77853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Buscando: 'What is vector search?'\n",
      "\n",
      "1. Similitud: 0.4186\n",
      "   Documento: Vector databases store embeddings for fast similarity search.\n",
      "\n",
      "2. Similitud: 0.2750\n",
      "   Documento: Semantic search finds documents based on meaning, not just keywords.\n",
      "\n",
      "3. Similitud: 0.2636\n",
      "   Documento: RAG combines retrieval of relevant documents with text generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar b√∫squeda sem√°ntica\n",
    "test_query = \"What is vector search?\"\n",
    "\n",
    "print(f\"üîç Buscando: '{test_query}'\\n\")\n",
    "results = semantic_search(test_query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Similitud: {result['similarity']:.4f}\")\n",
    "    print(f\"   Documento: {result['document']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f563a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n RAG completa\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n RAG completa\n",
    "def rag_response(query: str, top_k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Implementa RAG: busca documentos relevantes y genera respuesta\n",
    "    \"\"\"\n",
    "    # 1. RETRIEVAL: Buscar documentos relevantes\n",
    "    relevant_docs = semantic_search(query, top_k=top_k)\n",
    "\n",
    "     # 2. Construir contexto para el prompt\n",
    "    context = \"\\n\".join([doc[\"document\"] for doc in relevant_docs])\n",
    "    \n",
    "    # 3. GENERATION: Crear prompt con contexto\n",
    "    prompt = f\"\"\"Based on the following context, answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    # 4. Generar respuesta usando Cohere\n",
    "    response = co.chat(\n",
    "       message=prompt,\n",
    "       model=\"command-r-plus\",  # Modelo de chat de Cohere\n",
    "       temperature=0.3\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "print(\"Funci√≥n RAG completa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a4a69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Pregunta: What is RAG?\n",
      "ü§ñ Respuesta: RAG stands for Retrieval-Augmented Generation.\n",
      "\n",
      "--------------------------------------------------\n",
      "‚ùì Pregunta: How do embeddings work?\n",
      "ü§ñ Respuesta: Embeddings are created by converting text into numerical representations, which are then placed in a high-dimensional space.\n",
      "\n",
      "--------------------------------------------------\n",
      "‚ùì Pregunta: What are vector databases used for?\n",
      "ü§ñ Respuesta: Vector databases are used for storing embeddings, which are numerical representations of text, to enable fast similarity searches.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Probar RAG completo\n",
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"How do embeddings work?\",\n",
    "    \"What are vector databases used for?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"‚ùì Pregunta: {question}\")\n",
    "    answer = rag_response(question)\n",
    "    print(f\"ü§ñ Respuesta: {answer}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
