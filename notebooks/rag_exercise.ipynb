{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0a3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paulamoure/project/rag-exercise-cohere/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5d922bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere version: 5.15.0\n",
      "Pandas version: 2.2.3\n",
      "Numpy version: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "import cohere  # SDK oficial de Cohere para usar sus modelos (embeddings, chat, etc.)\n",
    "import pandas as pd  # Manipulaci√≥n de datos tabulares (DataFrames) - ideal para CSVs\n",
    "import numpy as np  # Operaciones matem√°ticas y arrays - necesario para embeddings\n",
    "import os  # Interactuar con el sistema operativo (variables de entorno, archivos)\n",
    "from typing import List, Dict  # Type hints para mejor c√≥digo (List[str], Dict[str, Any])\n",
    "import json  # Leer/escribir archivos JSON - √∫til para guardar embeddings\n",
    "\n",
    "# Verificar versiones\n",
    "print(f\"Cohere version: {cohere.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente Cohere configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "# Configurar API Key de forma segura\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Cargar API key desde variable de entorno\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"COHERE_API_KEY no encontrada en .env\")\n",
    "\n",
    "co = cohere.Client(api_key)\n",
    "print(\"Cliente Cohere configurado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32de27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n exitosa. Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Probar conexi√≥n con Cohere\n",
    "try:\n",
    "    # Test simple con un embedding peque√±o\n",
    "    response = co.embed(texts=[\"Hello world\"], model=\"embed-english-v3.0\", input_type=\"search_document\")\n",
    "    print(f\"‚úÖ Conexi√≥n exitosa. Embedding dimension: {len(response.embeddings[0])}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a975572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creado con 8 documentos\n",
      "0: Cohere provides large language models and NLP tools for businesses.\n",
      "1: RAG combines retrieval of relevant documents with text generation.\n",
      "2: Vector databases store embeddings for fast similarity search.\n",
      "3: Semantic search finds documents based on meaning, not just keywords.\n",
      "4: Embeddings are numerical representations of text in high-dimensional space.\n",
      "5: Machine learning models can understand context and generate human-like text.\n",
      "6: Natural language processing helps computers understand human language.\n",
      "7: AI assistants use retrieval-augmented generation for accurate responses.\n"
     ]
    }
   ],
   "source": [
    "# Crear dataset de documentos de ejemplo\n",
    "documents = [\n",
    "    \"Cohere provides large language models and NLP tools for businesses.\",\n",
    "    \"RAG combines retrieval of relevant documents with text generation.\",\n",
    "    \"Vector databases store embeddings for fast similarity search.\",\n",
    "    \"Semantic search finds documents based on meaning, not just keywords.\",\n",
    "    \"Embeddings are numerical representations of text in high-dimensional space.\",\n",
    "    \"Machine learning models can understand context and generate human-like text.\",\n",
    "    \"Natural language processing helps computers understand human language.\",\n",
    "    \"AI assistants use retrieval-augmented generation for accurate responses.\"\n",
    "]\n",
    "\n",
    "print(f\"Dataset creado con {len(documents)} documentos\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"{i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89fd73e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando embeddings para los documentos...\n",
      "Embeddings generados:\n",
      "- N√∫mero de documentos: 8\n",
      "- Dimensi√≥n de embeddings: 1024\n",
      "- Shape total: (8, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Generar embeddings para los documentos\n",
    "print(\"Generando embeddings para los documentos...\")\n",
    "\n",
    "# Crear embeddings para todos los documentos\n",
    "doc_embeddings = co.embed(\n",
    "    texts=documents,\n",
    "    model=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\"\n",
    ")\n",
    "\n",
    "# Convertir a numpy array para facilitar c√°lculos\n",
    "embeddings_array = np.array(doc_embeddings.embeddings)\n",
    "\n",
    "print(f\"Embeddings generados:\")\n",
    "print(f\"- N√∫mero de documentos: {len(documents)}\")\n",
    "print(f\"- Dimensi√≥n de embeddings: {embeddings_array.shape[1]}\")\n",
    "print(f\"- Shape total: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5736541d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n de b√∫squeda sem√°ntica creada\n"
     ]
    }
   ],
   "source": [
    "# Implementar b√∫squeda sem√°ntica\n",
    "def semantic_search(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Busca los documentos m√°s relevantes para una query usando similitud coseno\n",
    "    \"\"\"\n",
    "    # Generar embedding para la query\n",
    "    query_embedding = co.embed(\n",
    "        texts=[query],\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_query\"\n",
    "    )\n",
    "\n",
    "    query_vector = np.array(query_embedding.embeddings[0])\n",
    "\n",
    "    # Calcular similitud coseno con todos los documentos\n",
    "    similarities = np.dot(embeddings_array, query_vector) / (\n",
    "        np.linalg.norm(embeddings_array, axis=1) * np.linalg.norm(query_vector)\n",
    "    )\n",
    "\n",
    "    # Obtener los top_k m√°s similares\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"document\": documents[idx],\n",
    "            \"similarity\": similarities[idx],\n",
    "            \"index\": idx\n",
    "        })\n",
    "    return results\n",
    "print(\"Funci√≥n de b√∫squeda sem√°ntica creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c77853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Buscando: 'What is vector search?'\n",
      "\n",
      "1. Similitud: 0.4186\n",
      "   Documento: Vector databases store embeddings for fast similarity search.\n",
      "\n",
      "2. Similitud: 0.2750\n",
      "   Documento: Semantic search finds documents based on meaning, not just keywords.\n",
      "\n",
      "3. Similitud: 0.2636\n",
      "   Documento: RAG combines retrieval of relevant documents with text generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar b√∫squeda sem√°ntica\n",
    "test_query = \"What is vector search?\"\n",
    "\n",
    "print(f\"üîç Buscando: '{test_query}'\\n\")\n",
    "results = semantic_search(test_query, top_k=3)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Similitud: {result['similarity']:.4f}\")\n",
    "    print(f\"   Documento: {result['document']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f563a92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funci√≥n RAG completa\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n RAG completa\n",
    "def rag_response(query: str, top_k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Implementa RAG: busca documentos relevantes y genera respuesta\n",
    "    \"\"\"\n",
    "    # 1. RETRIEVAL: Buscar documentos relevantes\n",
    "    relevant_docs = semantic_search(query, top_k=top_k)\n",
    "\n",
    "     # 2. Construir contexto para el prompt\n",
    "    context = \"\\n\".join([doc[\"document\"] for doc in relevant_docs])\n",
    "    \n",
    "    # 3. GENERATION: Crear prompt con contexto\n",
    "    prompt = f\"\"\"Based on the following context, answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "    # 4. Generar respuesta usando Cohere\n",
    "    response = co.chat(\n",
    "       message=prompt,\n",
    "       model=\"command-r-plus\",  # Modelo de chat de Cohere\n",
    "       temperature=0.3\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "print(\"Funci√≥n RAG completa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a4a69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Pregunta: What is RAG?\n",
      "ü§ñ Respuesta: RAG stands for Retrieval-Augmented Generation.\n",
      "\n",
      "--------------------------------------------------\n",
      "‚ùì Pregunta: How do embeddings work?\n",
      "ü§ñ Respuesta: Embeddings are created by converting text into numerical representations, which are then placed in a high-dimensional space.\n",
      "\n",
      "--------------------------------------------------\n",
      "‚ùì Pregunta: What are vector databases used for?\n",
      "ü§ñ Respuesta: Vector databases are used for storing embeddings, which are numerical representations of text, to enable fast similarity searches.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Probar RAG completo\n",
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"How do embeddings work?\",\n",
    "    \"What are vector databases used for?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"‚ùì Pregunta: {question}\")\n",
    "    answer = rag_response(question)\n",
    "    print(f\"ü§ñ Respuesta: {answer}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1bc5535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando retrieval:\n",
      "Documento encontrado en posici√≥n 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funci√≥n para evaluar la calidad de retrieval\n",
    "def evaluate_retrieval(query: str, expected_doc_index: int, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Eval√∫a si el documento esperando est√° en los top_k resultados\n",
    "    \"\"\"\n",
    "    results = semantic_search(query, top_k=top_k)\n",
    "    retrieved_indices = [r[\"index\"] for r in results]\n",
    "\n",
    "    if expected_doc_index in retrieved_indices:\n",
    "        position = retrieved_indices.index(expected_doc_index) + 1\n",
    "        print(f\"Documento encontrado en posici√≥n {position}\")\n",
    "        return True, position\n",
    "    else:\n",
    "        print(f\"Documento esperado no encontrado en top-{top_k}\")\n",
    "        return False, -1\n",
    "    \n",
    "# Ejemplo de evaluaci√≥n\n",
    "print(\"Evaluando retrieval:\")\n",
    "evaluate_retrieval(\"What is semantic search?\", 3)  # El doc sobre semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "028f27da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings guardados en data/embeddings.json\n",
      "üí° En producci√≥n, esto evitar√≠a recalcular embeddings costosos\n",
      "üìÅ Archivo creado: 160051 bytes\n"
     ]
    }
   ],
   "source": [
    "# Guardar embeddings para evitar recalcular\n",
    "import json\n",
    "\n",
    "# Guardar embeddings y documentos\n",
    "data_to_save = {\n",
    "    \"documents\": documents,\n",
    "    \"embeddings\": embeddings_array.tolist(),  # Convertir numpy a lista para JSON\n",
    "    \"model_used\": \"embed-english-v3.0\"\n",
    "}\n",
    "\n",
    "# Guardar en archivo JSON\n",
    "with open(\"../data/embeddings.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Embeddings guardados en data/embeddings.json\")\n",
    "print(\"üí° En producci√≥n, esto evitar√≠a recalcular embeddings costosos\")\n",
    "\n",
    "\n",
    "# Verificar que se guard√≥ correctamente\n",
    "import os\n",
    "if os.path.exists(\"../data/embeddings.json\"):\n",
    "    file_size = os.path.getsize(\"../data/embeddings.json\")\n",
    "    print(f\"üìÅ Archivo creado: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f14fda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar embeddings guardados\n",
    "def load_saved_embeddings(filepath=\"../data/embeddings.json\"):\n",
    "    \"\"\"\n",
    "    Carga embeddings previamente guardados para evitar recalcular\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        documents = data[\"documents\"]\n",
    "        embeddings = np.array(data[\"embeddings\"])\n",
    "        model_used = data[\"model_used\"]\n",
    "\n",
    "        print(f\"‚úÖ Embeddings cargados desde {filepath}\")\n",
    "        print(f\"üìä {len(documents)} documentos, dimensi√≥n {embeddings.shape[1]}\")\n",
    "        print(f\"ü§ñ Modelo usado: {model_used}\")\n",
    "        return documents, embeddings\n",
    "    except FileNotFoundError:\n",
    "            print(f\"‚ùå Archivo {filepath} no encontrado\")\n",
    "            return None, None\n",
    "    # Probar la funci√≥n\n",
    "    loaded_docs, loaded_embeddings = load_saved_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e8d29b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ README.md actualizado con documentaci√≥n completa\n"
     ]
    }
   ],
   "source": [
    "# Generar documentaci√≥n del proyecto\n",
    "readme_content = \"\"\"\n",
    "# RAG Exercise with Cohere\n",
    "Este proyecto implementa un sistema RAG (Retrieval-Augmented Generation) b√°sico usando la API de Cohere.\n",
    "\n",
    "## Arquitectura RAG Implementada\n",
    "\n",
    "1. **Vectorizaci√≥n**: Conversi√≥n de documentos y queries a embeddings usando `embed-english-v3.0`\n",
    "2. **Almacenamiento**: Embeddings guardados en formato JSON para reutilizaci√≥n\n",
    "3. **Recuperaci√≥n**: B√∫squeda sem√°ntica por similitud coseno\n",
    "4. **Generaci√≥n**: Respuestas contextualizadas usando `command-r-plus`\n",
    "\n",
    "## Estructura del Proyecto\n",
    "rag-exercise-cohere/\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ rag_exercise.ipynb    # Notebook principal\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ embeddings.json       # Embeddings precalculados\n",
    "‚îú‚îÄ‚îÄ src/                      # C√≥digo reutilizable (futuro)\n",
    "‚îú‚îÄ‚îÄ .env                      # API keys (no incluido en Git)\n",
    "‚îú‚îÄ‚îÄ requirements.txt          # Dependencias Python\n",
    "‚îî‚îÄ‚îÄ README.md                # Esta documentaci√≥n\n",
    "\n",
    "## Tecnolog√≠as Usadas\n",
    "\n",
    "- **Cohere API**: Embeddings y generaci√≥n de texto\n",
    "- **NumPy**: C√°lculos vectoriales y similitud coseno\n",
    "- **Pandas**: Manipulaci√≥n de datos\n",
    "- **Python-dotenv**: Gesti√≥n segura de API keys\n",
    "\n",
    "## Resultados\n",
    "\n",
    "- ‚úÖ Sistema RAG funcional con 8 documentos de ejemplo\n",
    "- ‚úÖ B√∫squeda sem√°ntica con embeddings de 1024 dimensiones\n",
    "- ‚úÖ Generaci√≥n de respuestas contextualizadas\n",
    "- ‚úÖ Persistencia de embeddings para optimizaci√≥n\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "- [ ] Ampliar dataset con m√°s documentos\n",
    "- [ ] Implementar chunking para documentos largos\n",
    "- [ ] Agregar m√©tricas de evaluaci√≥n (BLEU, ROUGE)\n",
    "- [ ] Integrar con vector database (FAISS, ChromaDB)\n",
    "\"\"\"\n",
    "\n",
    "# Guardar README\n",
    "with open(\"../README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úÖ README.md actualizado con documentaci√≥n completa\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
